import pandas as pd



# Ver las primeras líneas del archivo para identificar el delimitador
with open('/home/reboot-student/code/Project-Transfer-Learning/tweets_politica_kaggle.csv', 'r') as file:
    for i in range(5):  # Mostrar las primeras 5 líneas
        print(file.readline())



import pandas as pd

csv_file_path = '/home/reboot-student/code/Project-Transfer-Learning/tweets_politica_kaggle.csv'

df = pd.read_csv(csv_file_path, delimiter='\t', on_bad_lines='skip')

df



df = df.drop(columns=['cuenta', 'timestamp'])
df


df_conteo = df.groupby('partido')['tweet'].count().reset_index()


import matplotlib.pyplot as plt

plt.pie(df_conteo['tweet'], labels=df_conteo['partido'], autopct='%1.1f%%')
plt.show()


import re
import unidecode
from tqdm import tqdm

def clean_tweet(tweet):
    tweet = tweet.lower()
    tweet = unidecode.unidecode(tweet)  # Normalizar caracteres con tildes
    tweet = re.sub(r'http\S+', '', tweet)  # Eliminar URLs
    tweet = re.sub(r'@\w+', '', tweet)  # Eliminar menciones
    tweet = re.sub(r'#\w+', '', tweet)  # Eliminar hashtags
    tweet = re.sub(r'[^\w\s]', '', tweet)  # Eliminar caracteres especiales
    return tweet.strip()

tqdm.pandas()
df['clean_tweet'] = df['tweet'].progress_apply(clean_tweet)


df[['partido', 'clean_tweet']].sample(5)


texts = df['clean_tweet']  # Cambia 'tweet_text' por el nombre correcto de la columna de texto
labels = df['partido']  # Cambia 'label' por el nombre de la columna de etiquetas

from transformers import DistilBertTokenizer

# Cargar el tokenizador
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenizar el texto
tokens = tokenizer(list(texts), padding=True, truncation=True, return_tensors="pt", max_length=512)



from transformers import DistilBertForSequenceClassification

# Modelo preentrenado con una capa de clasificación
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)  # Cambia num_labels según tu tarea



from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader, TensorDataset

# Dividir datos
train_texts, val_texts, train_labels, val_labels = train_test_split(
    tokens['input_ids'], labels, test_size=0.2, random_state=42
)

# Convertir a tensores
train_dataset = TensorDataset(train_texts, torch.tensor(train_labels.values))
val_dataset = TensorDataset(val_texts, torch.tensor(val_labels.values))

# Crear DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)



from torch.optim import AdamW
from torch.nn import CrossEntropyLoss
import torch

# Dispositivo
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Optimizer y Loss
optimizer = AdamW(model.parameters(), lr=5e-5)
loss_fn = CrossEntropyLoss()

# Entrenamiento
epochs = 3
for epoch in range(epochs):
    model.train()
    total_loss = 0
    for batch in train_loader:
        input_ids, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids=input_ids, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader)}")

